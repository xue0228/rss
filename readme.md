## 项目介绍

用于快速编写RSS爬虫并生成RSS格式文件，爬取过程使用协程。

## 安装依赖

将项目文件拷贝到本地后切换工作目录到当前项目，安装依赖库：

```
pip install -r requirements.txt
```

## 爬虫逻辑

首先给出一张流程图，这是笔者使用协程进行简易爬虫框架编写的思路：

![img](https://gitee.com/xue_yi_wei/picture/raw/master/img/dce194a1602bfdf72e1112e45cb7fff6.jpg)

框架中涉及的几个爬虫关键步骤基本是套用的scrapy的思路，当然具体的实现逻辑基本没啥关系。

上面的流程图看着复杂，其实基本思路很简单，首先创建三个队列，分别存放请求信息、请求响应以及从响应中解析出来的数据；接着在主线程中创建一个子线程，在子线程中运行一个不断循环的协程任务循环，用于处理协程任务；主线程中也有一个循环，不断检测三个队列中的任务是否为空，不为空就将队列中的任务取出，分配不同的函数创建协程任务加入到协程循环中；待所有的队列为空，且协程循环中没有协程任务时结束主线程。

下面介绍一下Spider类中用户可以自定义的方法和属性：

- start_urls：必须属性，用于存储最初的爬取链接。
- retry_times：可选属性，默认为10，用于规定Request和Item处理失败后的重试次数。
- request_delay：可选属性，默认为0，用于规定两次Request之间的最短时间间隔，当需要对同一域名发起大量请求时建议设置恰当的时间间隔，以防触发反爬机制。
- parse：必须方法，用于处理请求成功后得到的响应数据，可以在此函数中向Request或其他队列中加入新的任务，同时此方法中返回的字典数据会自动转为Item对象加入Item队列等待处理。
- item_pipeline：必须方法，用于处理Item对象，可以在此处进行一些数据存储工作，例如保存到文件、写入数据库等。
- init：可选方法，此方法运行于所有其他方法之前，用于一些初始化设置，可以在此定义一些属性用于存储全局数据，或是进行一些登录操作等。
- request_filter_rule：可选方法，从request_queue中取出Request后即使用此方法进行筛选，返回False的Request将会被拦截。
- request_middlewares：可选方法，对Request进行实际请求之前运行，用于对Request进行一些额外的设置，例如添加代理，添加请求头等。
- response_filter_rule：可选方法，从response_queue中取出Response后即使用此方法进行筛选，默认为状态码2开头的Response可以通过，返回False的Response将会被拦截。
- response_middlewares：可选方法，在parse方法运行之前运行，可以对Response进行一些自定义的处理，例如给不同层级的Response添加不同的标签，方便后续处理时分辨。

## RSS爬虫编写示例

下面会以爬取B站Up主视频更新为例进行演示：



